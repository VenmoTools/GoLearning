# 内存相关概念
## 内存分页管理
### 什么是页？
将虚拟内存空间和物理内存空间按照某种规定的大小进行分配成为页(Page)
### 为什么要内存分页？
分页管理将虚拟内存空间和物理内存空间皆划分为大小相同的页面，并以页面作为内存空间的最小分配单位，一个程序的一个页面可以存放在任意一个物理页面里。
#### 内存空间碎片化
每次程序按照页进行内存分配，也就克服了外部碎片的问题。
#### 解决程序大小受限
程序将运行时所需要的页加载进内存，暂时不需要的页放在磁盘上，如果程序需要更多空间，只需再分配一个页即可

### 虚拟地址
#### 虚拟地址和物理地址

##### 物理地址
>放在寻址总线上的地址。放在寻址总线上，如果是读，电路根据这个地址每位的值就将相应地址的物理内存中的数据放到数据总线中传输。如果是写，电路根据这个地址每位的值就在相应地址的物理内存中放入数据总线上的内容。物理内存是以字节(8位)为单位编址的 <i>--摘自百度百科</i>

##### 虚拟地址
> 程序运行在虚拟地址空间中，如果CPU寄存器中的分页标志位被设置，那么执行内存操作的机器指令时MMU（内存管理单元）自动根据页目录和页表中的信息，把虚拟地址转换成物理地址，完成该指令
比如 mov eax,[004227b8h] ，这是把地址004227b8h处的值赋给寄存器的汇编代码，004227b8这个地址就是虚拟址。CPU在执行这行代码时，发现寄存器中的分页标志位已经被设定，就自动完成虚拟地址到物理地址的转换，使用物理地址取出值，完成指令。    <i>--摘自百度百科</i>

# GO内存分配
分配器将内存快分为两种
+ span:由多个连续的页组成的大内存块
+ object:将span按照特定大小切分多个小块

分配器按照按页数区分span，以页数为单位存放在管理数组中，需要时按照索引查找，获取span时如果没有合适大小则返回页数更多的span，然后进行裁剪操作，多余的部分将构成新的span放在管理数组中，分配器还尝试将地址相邻的span合并，以构建更大内存

go的内存分配操作的代码放在runtime/malloc.go文件中

分配器的数据结构为
+ fixalloc: 分配器为了方便管理和存储，将固定的非堆存储的object将在可利用空间表(free-list)
+ mspan: 运行由mheap管理的页
+ mheap: 内存分配堆，管理页的粒度（8192字节），用于管理闲置的span，需要时向操作系统申请新内存
+ mcache: 每个 Gorontine 的运行都是绑定到一个 P 上面,mcache 是每个 P 的 cache
+ mcentral: 全局 cache，mcache 不够用的时候向 mcentral 申请
+ mstats:分配统计


## fixalloc结构
fixalloc用于分配固定大小的object，Malloc使用fixalloc进行封装，管理自己的MCache和MSpan objects

FixAlloc是一个简单的自由列表分配器，用于固定大小的对象.Malloc使用一个固定在sysAlloc上的FixAlloc来管理它的mcache和mspan对象。

fixalloc.alloc的方法返回的内存默认为0，但是调用者可以通过设置标志位为false，如果内存永远不包含堆指针因此是安全的

调用方负责锁定FixAlloc调用。

调用方可以在对象中保持状态，但第一个单词是通过释放和重新分配来粉碎的。 // WTF？？？原文是The caller is responsible for locking around FixAlloc calls. Callers can keep state in the object but the first word is smashed by freeing and reallocating.

```go
type fixalloc struct {
	size   uintptr
	first  func(arg, p unsafe.Pointer) // 第一次调用p返回
	arg    unsafe.Pointer
	list   *mlink // list为一个mlink链表
	chunk  uintptr // 当前分配的内存块，使用uintptr而不是unsafe.Pointer来避免写入障碍
	nchunk uint32 
	inuse  uintptr // 现在使用中的字节
	stat   *uint64
	zero   bool // zero allocations
}
//块的通用链接列表。 （通常，块大于sizeof（MLink）。）由于对mlink.next的赋值将导致执行写屏障，因此某些内部GC结构无法使用此屏障。例如，当清扫器将未标记的对象放置在空闲列表上时，它不希望调用写入屏障，因为这可能导致对象可达。
type mlink struct {
	next *mlink
}

//初始化fixalloc以分配给定大小的对象，使用分配器获取内存块
func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *uint64) {
	f.size = size
	f.first = first
	f.arg = arg
	f.list = nil
	f.chunk = 0
	f.nchunk = 0
	f.inuse = 0
	f.stat = stat
	f.zero = true
}
func (f *fixalloc) alloc() unsafe.Pointer {
	if f.size == 0 {
		print("runtime: use of FixAlloc_Alloc before FixAlloc_Init\n")
		throw("runtime: internal error")
	}

	if f.list != nil { // 如果当前链表不为nil
		v := unsafe.Pointer(f.list) //获取链表首地址
		f.list = f.list.next //获取将链表的下一块作为当前的内存块
		f.inuse += f.size  // 重新统计已分配大小
		if f.zero { // 如果当前内存块需要0值，则调用memclrNoHeapPointers零初始化
			memclrNoHeapPointers(v, f.size)
		}
		return v
	}
	if uintptr(f.nchunk) < f.size { // 如果当前nchunk不足以分配其大小，则申请一个
		f.chunk = uintptr(persistentalloc(_FixAllocChunk, 0, f.stat)) 
		f.nchunk = _FixAllocChunk
	}

	v := unsafe.Pointer(f.chunk)
	if f.first != nil {
		f.first(f.arg, v) 
	}
	f.chunk = f.chunk + f.size
	f.nchunk -= uint32(f.size)
	f.inuse += f.size
	return v
}

func (f *fixalloc) free(p unsafe.Pointer) {
	f.inuse -= f.size
	v := (*mlink)(p)
	v.next = f.list
	f.list = v
}

//sysAlloc周围的包装器，可以分配小块。没有相关的释放操作。用于函数/类型/调试相关的持久数据。如果align为0，则使用默认对齐（当前为8）。返回的内存将被清零。考虑标记persistentalloc'd类型go：notinheap。
func persistentalloc(size, align uintptr, sysStat *uint64) unsafe.Pointer {
	var p *notInHeap
	systemstack(func() {
		p = persistentalloc1(size, align, sysStat)
	})
	return unsafe.Pointer(p)
}
```

## mcache结构

```go
type mcache struct {
	// The following members are accessed on every malloc,
	// so they are grouped here for better caching.
	next_sample int32   // trigger heap sample after allocating this many bytes
	local_scan  uintptr // bytes of scannable heap allocated

	// Allocator cache for tiny objects w/o pointers.
	// See "Tiny allocator" comment in malloc.go.

	// tiny points to the beginning of the current tiny block, or
	// nil if there is no current tiny block.
	//
	// tiny is a heap pointer. Since mcache is in non-GC'd memory,
	// we handle it by clearing it in releaseAll during mark
	// termination.
	tiny             uintptr //小对象分配器，小于16byte的对象将会通过tiny分配
	tinyoffset       uintptr
	local_tinyallocs uintptr // number of tiny allocs not counted in other stats

	// The rest is not accessed on every malloc.

	alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass

	stackcache [_NumStackOrders]stackfreelist

	// Local allocator stats, flushed during GC.
	local_largefree  uintptr                  // bytes freed for large objects (>maxsmallsize)
	local_nlargefree uintptr                  // number of frees for large objects (>maxsmallsize)
	local_nsmallfree [_NumSizeClasses]uintptr // number of frees for small objects (<=maxsmallsize)
}
```

alloc [numSpanClasses]*mspan是一个大小为67的指针，指向mspan的数组，每个数组元素用来包含特定大小的块。当要分配内存大小时，为 object 在 alloc 数组中选择合适的元素来分配

相关定义在runtime/sizeclasses.go
```go
var class_to_size = [_NumSizeClasses]uint16{0, 8, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 896, 1024, 1152, 1280, 1408, 1536, 1792, 2048, 2304, 2688, 3072, 3200, 3456, 4096, 4864, 5376, 6144, 6528, 6784, 6912, 8192, 9472, 9728, 10240, 10880, 12288, 13568, 14336, 16384, 18432, 19072, 20480, 21760, 24576, 27264, 28672, 32768}
```

## mspan结构
相关定义在runtime/mheap.go
```go
type mspan struct {
    // 指针域，mspan 一般都是以链表形式使用
	next *mspan     // next span in list, or nil if none
	prev *mspan     // previous span in list, or nil if none
	list *mSpanList // For debugging. TODO: Remove.

	startAddr uintptr // address of first byte of span aka s.base()
	npages    uintptr // mspan 的大小为 page 大小的整数倍

	manualFreeList gclinkptr // list of free objects in _MSpanManual spans

	// freeindex is the slot index between 0 and nelems at which to begin scanning
	// for the next free object in this span.
	// Each allocation scans allocBits starting at freeindex until it encounters a 0
	// indicating a free object. freeindex is then adjusted so that subsequent scans begin
	// just past the newly discovered free object.
	//
	// If freeindex == nelem, this span has no free objects.
	//
	// allocBits is a bitmap of objects in this span.
	// If n >= freeindex and allocBits[n/8] & (1<<(n%8)) is 0
	// then object n is free;
	// otherwise, object n is allocated. Bits starting at nelem are
	// undefined and should never be referenced.
	//
	// Object n starts at address n*elemsize + (start << pageShift).
	freeindex uintptr
	// TODO: Look up nelems from sizeclass and remove this field if it
	// helps performance.
	nelems uintptr // number of object in the span.

	// Cache of the allocBits at freeindex. allocCache is shifted
	// such that the lowest bit corresponds to the bit freeindex.
	// allocCache holds the complement of allocBits, thus allowing
	// ctz (count trailing zero) to use it directly.
	// allocCache may contain bits beyond s.nelems; the caller must ignore
	// these.
	allocCache uint64

	// allocBits and gcmarkBits hold pointers to a span's mark and
	// allocation bits. The pointers are 8 byte aligned.
	// There are three arenas where this data is held.
	// free: Dirty arenas that are no longer accessed
	//       and can be reused.
	// next: Holds information to be used in the next GC cycle.
	// current: Information being used during this GC cycle.
	// previous: Information being used during the last GC cycle.
	// A new GC cycle starts with the call to finishsweep_m.
	// finishsweep_m moves the previous arena to the free arena,
	// the current arena to the previous arena, and
	// the next arena to the current arena.
	// The next arena is populated as the spans request
	// memory to hold gcmarkBits for the next GC cycle as well
	// as allocBits for newly allocated spans.
	//
	// The pointer arithmetic is done "by hand" instead of using
	// arrays to avoid bounds checks along critical performance
	// paths.
	// The sweep will free the old allocBits and set allocBits to the
	// gcmarkBits. The gcmarkBits are replaced with a fresh zeroed
	// out memory.
	allocBits  *gcBits
	gcmarkBits *gcBits

	// sweep generation:
	// if sweepgen == h->sweepgen - 2, the span needs sweeping
	// if sweepgen == h->sweepgen - 1, the span is currently being swept
	// if sweepgen == h->sweepgen, the span is swept and ready to use
	// h->sweepgen is incremented by 2 after every GC

	sweepgen    uint32
	divMul      uint16     // for divide by elemsize - divMagic.mul
	baseMask    uint16     // if non-0, elemsize is a power of 2, & this will get object allocation base
	allocCount  uint16     // number of allocated objects
	spanclass   spanClass  // size class and noscan (uint8)
	incache     bool       // being used by an mcache
	state       mSpanState // mspaninuse etc
	needzero    uint8      // needs to be zeroed before allocation
	divShift    uint8      // for divide by elemsize - divMagic.shift
	divShift2   uint8      // for divide by elemsize - divMagic.shift2
	elemsize    uintptr    // computed from sizeclass or from npages
	unusedsince int64      // first time spotted by gc in mspanfree state
	npreleased  uintptr    // number of pages released to the os
	limit       uintptr    // end of data in span
	speciallock mutex      // guards specials list
	specials    *special   // linked list of special records sorted by offset.
}

```


+ next, prev: 指针域，mspan 一般都是以链表形式使用。
+ npages: mspan 的大小为 page 大小的整数倍。
+ spanclass: 0 ~ _NumSizeClasses 之间的一个值,比如，sizeclass = 3，那么这个 mspan 被分割成 32 byte 的块。
+ nelems: span 中包块的总数目。
+ freeindex: 0 ~ nelemes-1，表示分配到第几个块。
+ elemsize: 通过 sizeclass 或者 npages 可以计算出来。比如 sizeclass = 3, elemsize = 32 byte。对于大于 32Kb 的内存分配，都是分配整数页，elemsize = page_size * npages。

对于存储对象的object，按照8字节的倍数分为n种，例如一个24大小的object可以存储范围在17-24字节的对象

分配器初始化时会构建对照表，存储大小和规格的映射，如果对象超过特定的阀值，会被当作大对象对待

```go
_MaxSmallSize = 32 << 10 //32kb
```



## mcentral结构

```go
type mcentral struct {
	lock      mutex
	spanclass spanClass
	nonempty  mSpanList // mspan 的双向链表，当前 mcentral 中可用的 mspan list
	empty     mSpanList // 已经被使用的

	// nmalloc is the cumulative count of objects allocated from
	// this mcentral, assuming all spans in mcaches are
	// fully-allocated. Written atomically, read under STW.
	nmalloc uint64
}
```


## mheap结构

```go
type mheap struct {
	lock      mutex
	free      [_MaxMHeapList]mSpanList // 页数在_MaxMHeapList(127)以内的闲置span链表
	freelarge mTreap                   // 页数 >= _MaxMHeapList闲置的span链表
	busy      [_MaxMHeapList]mSpanList // 页数在_MaxMHeapList(127)以内的使用的span链表
	busylarge mSpanList                // 页数 >= _MaxMHeapList使用的span链表
	sweepgen  uint32                   // sweep generation, see comment in mspan
	sweepdone uint32                   // all spans are swept
	sweepers  uint32                   // number of active sweepone calls

	// allspans is a slice of all mspans ever created. Each mspan
	// appears exactly once.
	//
	// The memory for allspans is manually managed and can be
	// reallocated and move as the heap grows.
	//
	// In general, allspans is protected by mheap_.lock, which
	// prevents concurrent access as well as freeing the backing
	// store. Accesses during STW might not hold the lock, but
	// must ensure that allocation cannot happen around the
	// access (since that may free the backing store).
	allspans []*mspan // all spans out there

	// sweepSpans contains two mspan stacks: one of swept in-use
	// spans, and one of unswept in-use spans. These two trade
	// roles on each GC cycle. Since the sweepgen increases by 2
	// on each cycle, this means the swept spans are in
	// sweepSpans[sweepgen/2%2] and the unswept spans are in
	// sweepSpans[1-sweepgen/2%2]. Sweeping pops spans from the
	// unswept stack and pushes spans that are still in-use on the
	// swept stack. Likewise, allocating an in-use span pushes it
	// on the swept stack.
	sweepSpans [2]gcSweepBuf

	//_ uint32 // align uint64 fields on 32-bit for atomics

	// Proportional sweep
	//
	// These parameters represent a linear function from heap_live
	// to page sweep count. The proportional sweep system works to
	// stay in the black by keeping the current page sweep count
	// above this line at the current heap_live.
	//
	// The line has slope sweepPagesPerByte and passes through a
	// basis point at (sweepHeapLiveBasis, pagesSweptBasis). At
	// any given time, the system is at (memstats.heap_live,
	// pagesSwept) in this space.
	//
	// It's important that the line pass through a point we
	// control rather than simply starting at a (0,0) origin
	// because that lets us adjust sweep pacing at any time while
	// accounting for current progress. If we could only adjust
	// the slope, it would create a discontinuity in debt if any
	// progress has already been made.
	pagesInUse         uint64  // pages of spans in stats _MSpanInUse; R/W with mheap.lock
	pagesSwept         uint64  // pages swept this cycle; updated atomically
	pagesSweptBasis    uint64  // pagesSwept to use as the origin of the sweep ratio; updated atomically
	sweepHeapLiveBasis uint64  // value of heap_live to use as the origin of sweep ratio; written with lock, read without
	sweepPagesPerByte  float64 // proportional sweep ratio; written with lock, read without
	// TODO(austin): pagesInUse should be a uintptr, but the 386
	// compiler can't 8-byte align fields.

	// Malloc stats.
	largealloc  uint64                  // bytes allocated for large objects
	nlargealloc uint64                  // number of large object allocations
	largefree   uint64                  // bytes freed for large objects (>maxsmallsize)
	nlargefree  uint64                  // number of frees for large objects (>maxsmallsize)
	nsmallfree  [_NumSizeClasses]uint64 // number of frees for small objects (<=maxsmallsize)

	// arenas是arena堆映射。它指向整个可用虚拟地址空间的每个arena帧的堆的元数据。使用arenaIndex计算此数组的索引。对于Go堆未支持的地址空间区域，arena映射包含nil。修改受mheap_.lock保护。读取可以在没有锁定的情况下执行;但是，在没有保持锁定的任何时候，给定条目可以从零转换为非零。 （条目永远不会转换为零。）通常，这是一个由L1映射和可能许多L2映射组成的两级映射。当存在大量的arena帧时，这节省了空间。但是，在许多平台（甚至64位）上，arenaL1Bits为0，使其有效地成为单级映射。在这种情况下，arenas [0]永远不会是零
	arenas [1 << arenaL1Bits]*[1 << arenaL2Bits]*heapArena

	// heapArenaAlloc是用于分配heapArenaobjects的预留空间。这仅用于32位，我们预先保留此空间以避免将其与堆本身交错
	heapArenaAlloc linearAlloc

	// arenaHints是尝试添加更多arenas堆的地址列表。这最初填充了一组通用提示地址，并以实际arena堆范围的边界增长。
	arenaHints *arenaHint

	// arena是一个预留空间，用于分配堆arenas（实际的arenas）。这仅用于32位
	arena linearAlloc

	//_ uint32 // ensure 64-bit alignment of central

	// central free lists for small size classes.
	// the padding makes sure that the MCentrals are
	// spaced CacheLineSize bytes apart, so that each MCentral.lock
	// gets its own cache line.
	// central is indexed by spanClass.
	central [numSpanClasses]struct {
		mcentral mcentral
		pad      [sys.CacheLineSize - unsafe.Sizeof(mcentral{})%sys.CacheLineSize]byte
	} //每一个central对应一个spanclass

	spanalloc             fixalloc // 分配span的fixalloc
	cachealloc            fixalloc // 分配cache的fixalloc
	treapalloc            fixalloc // 为大对象分配树枝节点的fixalloc
	specialfinalizeralloc fixalloc // allocator for specialfinalizer*
	specialprofilealloc   fixalloc // allocator for specialprofile*
	speciallock           mutex    // lock for special record allocators.
	arenaHintAlloc        fixalloc // allocator for arenaHints

	unused *specialfinalizer // never set, just here to force the specialfinalizer type into DWARF
}
```

mheap_ 是一个全局变量，会在系统初始化的时候初始化（在函数 mallocinit() 中）
+ free      [_MaxMHeapList]mSpanList：这是一个 SpanList 数组，每个 SpanList 里面的 mspan 由 1 ~ 127 (_MaxMHeapList - 1) 个 page 组成。比如 free[3] 是由包含 3 个 page 的 mspan 组成的链表
+ allspans []*mspan: 所有的 spans 都是通过 mheap_ 申请，所有申请过的 mspan 都会记录在 allspans。结构体中的 lock 就是用来保证并发安全的
+ central [numSpanClasses]struct {
		mcentral mcentral
		pad      [sys.CacheLineSize - unsafe.Sizeof(mcentral{})%sys.CacheLineSize]byte
	}:每种大小的块对应一个 pad 可以认为是一个字节填充，为了避免伪共享（false sharing）问题的
+ sweepgen, sweepdone: GC 相关
+ spanalloc, cachealloc fixalloc: fixalloc 是 free-list，用来分配特定大小的块
+ arenas [1 << arenaL1Bits]*[1 << arenaL2Bits]*heapArena： arenas是arena堆映射。它指向整个可用虚拟地址空间的每个arena帧的堆的元数据。使用arenaIndex计算此数组的索引
+ heapArenaAlloc linearAlloc：heapArenaAlloc是用于分配heapArenaobjects的预留空间，linearAlloc是一个简单的线性分配器，它预先保留一个内存区域，然后根据需要映射该区域。调用者负责锁定。结构如下
```go
type linearAlloc struct {
	next   uintptr // 下一个空闲字节
	mapped uintptr // 映射空间结束后的一个字节
	end    uintptr // 预留空间的结束
}
```
+ arenaHints *arenaHint： arenaHints是尝试添加更多arenas堆的地址链表
+ spans []*mspan: 记录 arena 区域页号（page number）和 mspan 的映射关系
+ arena_start, arena_end, arena_used：
arena 是 Golang 中用于分配内存的连续虚拟地址区域。由 mheap 管理，堆上申请的所有内存都来自 arena，内存布局如下

```
+------------+----------------+------------------------------------+
|span 512mb  | bitmap  32gb   |  arena 512gb                       |   
+------------+----------------+------------------------------------+
|span_mapped | bitmap_mapped  |  arena_start  arena_used  arena_end|
+------------+----------------+------------------------------------+
```

#### 内存分配流程
1. 计算待分配对象所对应的规格
2. 从mcache.alloc数组中查找规格相同的object
3. 从mspan.freelist链表中提取可用的object
4. 若mspan.freelist为空，从mcentral更新获取mspan
5. 若mcentral.nonempty为空，从mheap.free/freelarge中获取并切分成object链表
6. 若heap没有大小合适的闲置span，向操作系统申请新的内存块

#### 内存释放流程
1. 将标记为可回收的object交还给mspan.freelist
2. 该mspan被释放会central，可供任意的mcache重新获取使用
3. 若mspan以回收全部object，则交还给mheap，以便重新切分
4. 定期扫描heap里长时间闲置的span，释放占用的内存

#### 分配流程（malloc.go文件中的说明）
1. 调整小对象，并查看p中mcache对应的mspan，扫描空的bitmap以找到空的slot，如果有空闲的则分配，这个过程可以在不分配锁的情况下进行
2. 如果没有空的slot，则从mcentral中获取所需大小，空闲的mspan，获取整个span以减少锁住mcentral的成本
3. 如果mcentral中的spanlist为空，则从mheap中获取mspan所需的一组页
4. 如果mheap为空或没有足够空间的页，则向操作系统获取一组页(至少1mb)，分配大量页面会减轻与操作系统通信的成本。

仅当mspan.needzero为false时，mspan中的空闲对象槽才会归零。如果needzero为true，则对象在分配时归零。以这种方式延迟归零有各种好处
1. 堆栈帧分配可以完全避免归零
2. 它表现出更好的时间局部性，因为程序可能要写入内存。
3. 不会让零页面永远不会被重用。

# 初始化

mallocinit函数
```go

func mallocinit() {
	// 环境检测
	if class_to_size[_TinySizeClass] != _TinySize {
		throw("bad TinySizeClass")
	}

	testdefersizes()

	if heapArenaBitmapBytes&(heapArenaBitmapBytes-1) != 0 {
		// heapBits expects modular arithmetic on bitmap
		// addresses to work.
		throw("heapArenaBitmapBytes not a power of 2")
	}

	// Copy class sizes out for statistics table.
	for i := range class_to_size {
		memstats.by_size[i].size = uint32(class_to_size[i])
	}

	// Check physPageSize.
	if physPageSize == 0 {
		// The OS init code failed to fetch the physical page size.
		throw("failed to get system page size")
	}
	if physPageSize < minPhysPageSize {
		print("system page size (", physPageSize, ") is smaller than minimum page size (", minPhysPageSize, ")\n")
		throw("bad system page size")
	}
	if physPageSize&(physPageSize-1) != 0 {
		print("system page size (", physPageSize, ") must be a power of 2\n")
		throw("bad system page size")
	}

	// 初始化mheap
	// Initialize the heap.
	mheap_.init()
	_g_ := getg()
	_g_.m.mcache = allocmcache()

	// 判断指针大小，如果为8则代表这是64位系统
	// Create initial arena growth hints.
	if sys.PtrSize == 8 && GOARCH != "wasm" {
		// 在64位机器上，我们将遵循以下约定，因为
		// 1.从地址空间的中间开始，可以更容易地扩展到连续的范围，而无需运行到其他映射
		// 2. 这使得Go堆地址在调试时更容易识别。
		// 3. gccgo中的堆栈扫描仍然是保守的，因此地址与其他数据的区别
		//
		// 从0x00c0开始意味着有效的内存地址，0x00c0,0x00c1...
		// 在little-endian中，那是c0 00，c1 00，......这些都不是有效的
		// UTF-8序列，它们尽可能远离FF（可能是一个相同字节）。如果失败，尝试其他0xXXc0
		// 早先尝试使用0x11f8导致内存不足错误
		// 在线程分配期间在OS X上。 0x00c0导致冲突
		// AddressSanitizer，保留所有内存，最大为0x0100。
		// 这些选择降低了保守垃圾收集器的可能性
		// 没有收集内存，因为一些非指针内存块
		// had a bit pattern that matched a memory address.
		//
		// 但是，在arm64上，我们忽略了上述所有这些建议并将分配大小调整为0x40 << 32因为当使用带有3级转换缓冲区的4k页时，用户地址空间限制为39位在darwin / arm64上，地址空间更小
		for i := 0x7f; i >= 0; i-- {
			var p uintptr
			switch {
			case GOARCH == "arm64" && GOOS == "darwin":
				p = uintptr(i)<<40 | uintptrMask&(0x0013<<28)
			case GOARCH == "arm64":
				p = uintptr(i)<<40 | uintptrMask&(0x0040<<32)
			case raceenabled:
				// The TSAN runtime requires the heap
				// to be in the range [0x00c000000000,
				// 0x00e000000000).
				p = uintptr(i)<<32 | uintptrMask&(0x00c0<<32)
				if p >= uintptrMask&0x00e000000000 {
					continue
				}
			default:
				p = uintptr(i)<<40 | uintptrMask&(0x00c0<<32)
			}
			hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc())
			hint.addr = p
			hint.next, mheap_.arenaHints = mheap_.arenaHints, hint
		}
	} else {
		// On a 32-bit machine, we're much more concerned
		// about keeping the usable heap contiguous.
		// Hence:
		//
		// 1. We reserve space for all heapArenas up front so
		// they don't get interleaved with the heap. They're
		// ~258MB, so this isn't too bad. (We could reserve a
		// smaller amount of space up front if this is a
		// problem.)
		//
		// 2. We hint the heap to start right above the end of
		// the binary so we have the best chance of keeping it
		// contiguous.
		//
		// 3. We try to stake out a reasonably large initial
		// heap reservation.

		const arenaMetaSize = unsafe.Sizeof([1 << arenaBits]heapArena{})
		meta := uintptr(sysReserve(nil, arenaMetaSize))
		if meta != 0 {
			mheap_.heapArenaAlloc.init(meta, arenaMetaSize)
		}

		// We want to start the arena low, but if we're linked
		// against C code, it's possible global constructors
		// have called malloc and adjusted the process' brk.
		// Query the brk so we can avoid trying to map the
		// region over it (which will cause the kernel to put
		// the region somewhere else, likely at a high
		// address).
		procBrk := sbrk0()

		// If we ask for the end of the data segment but the
		// operating system requires a little more space
		// before we can start allocating, it will give out a
		// slightly higher pointer. Except QEMU, which is
		// buggy, as usual: it won't adjust the pointer
		// upward. So adjust it upward a little bit ourselves:
		// 1/4 MB to get away from the running binary image.
		p := firstmoduledata.end
		if p < procBrk {
			p = procBrk
		}
		if mheap_.heapArenaAlloc.next <= p && p < mheap_.heapArenaAlloc.end {
			p = mheap_.heapArenaAlloc.end
		}
		p = round(p+(256<<10), heapArenaBytes)
		// Because we're worried about fragmentation on
		// 32-bit, we try to make a large initial reservation.
		arenaSizes := []uintptr{
			512 << 20,
			256 << 20,
			128 << 20,
		}
		for _, arenaSize := range arenaSizes {
			a, size := sysReserveAligned(unsafe.Pointer(p), arenaSize, heapArenaBytes)
			if a != nil {
				mheap_.arena.init(uintptr(a), size)
				p = uintptr(a) + size // For hint below
				break
			}
		}
		hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc())
		hint.addr = p
		hint.next, mheap_.arenaHints = mheap_.arenaHints, hint
	}
}
```

## heap

heap初始化相关代码
```go
mheap_.init() // runtime.malloc.go

...
// runtime.mheap.go
// 初始化mheap
func (h *mheap) init() {
	h.treapalloc.init(unsafe.Sizeof(treapNode{}), nil, nil, &memstats.other_sys) // 初始化treapalloc
	h.spanalloc.init(unsafe.Sizeof(mspan{}), recordspan, unsafe.Pointer(h), &memstats.mspan_sys) // mspan初始化
	h.cachealloc.init(unsafe.Sizeof(mcache{}), nil, nil, &memstats.mcache_sys)
	h.specialfinalizeralloc.init(unsafe.Sizeof(specialfinalizer{}), nil, nil, &memstats.other_sys) // mcache初始化
	h.specialprofilealloc.init(unsafe.Sizeof(specialprofile{}), nil, nil, &memstats.other_sys) //specialprofile初始化 specialprofile所描述的对象具有为其设置的析构器
	h.arenaHintAlloc.init(unsafe.Sizeof(arenaHint{}), nil, nil, &memstats.other_sys) // arenaHint初始化 arenaHints是尝试添加更多arenas堆的地址列表

	// 不要将mspan分配为零。Background sweeping可以在分配范围的同时检查跨度，因此跨度的扫描在跨越自由并重新分配跨度以避免背景扫除从0不正确地进行扫描是很重要的
	// mspan不包含堆指针因此是安全的
	h.spanalloc.zero = false

	// h->mapcache 不用初始化
	for i := range h.free {
		h.free[i].init() // 初始化空闲mspan表
		h.busy[i].init() // 初始化已使用的mspan表
	}

	h.busylarge.init() 
	for i := range h.central { // 初始化central中的所有mcentral spanClass为uint8的类型别名
		h.central[i].mcentral.init(spanClass(i))
	}
}

```

## 初始化arenaHits

```go
for i := 0x7f; i >= 0; i-- {
	var p uintptr
	switch {
	case GOARCH == "arm64" && GOOS == "darwin":
		p = uintptr(i)<<40 | uintptrMask&(0x0013<<28)
	case GOARCH == "arm64":
		p = uintptr(i)<<40 | uintptrMask&(0x0040<<32)
	case raceenabled:
		//TSAN运行时需要堆 [0x00c000000000,0x00e000000000).
		p = uintptr(i)<<32 | uintptrMask&(0x00c0<<32)
		if p >= uintptrMask&0x00e000000000 {
			continue
		}
	default:
		p = uintptr(i)<<40 | uintptrMask&(0x00c0<<32)
	}
	hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc())
	hint.addr = p
	hint.next, mheap_.arenaHints = mheap_.arenaHints, hint
}
```

## 初始化流程
1. 创建对象规格大小映射
2. 计算相关区域大小，并尝试从某个指定位置开始保留地址空间
3. 初始化mheap和其他属性

## 保留地址空间
file: runtime.mem_linux.go

```go
func sysReserve(v unsafe.Pointer, n uintptr) unsafe.Pointer {
	p, err := mmap(v, n, _PROT_NONE, _MAP_ANON|_MAP_PRIVATE, -1, 0)
	if err != 0 {
		return nil
	}
	return p
}
```
mmap要求内核创建新的虚拟存储器区域，可制定其实地址和长度，windows没有此函数，对应的API为VirtualAlloc

其中：
+ PROT_EXEC //页内容可以被执行
+ PROT_READ //页内容可以被读取
+ PROT_WRITE //页可以被写入
+ PROT_NONE //页不可访问
+ MAP_FIXED //使用指定的映射起始地址，如果由start和len参数指定的内存区重叠于现存的映射空间，重叠部分将会被丢弃。如果指定的起始地址不可用，操作将会失败。并且起始地址必须落在页的边界上。
+ MAP_SHARED //与其它所有映射这个对象的进程共享映射空间。对共享区的写入，相当于输出到文件。直到msync()或者munmap()被调用，文件实际上不会被更新。
+ MAP_PRIVATE //建立一个写入时拷贝的私有映射。内存区域的写入不会影响到原文件。这个标志和以上标志是互斥的，只能使用其中一个。
+ MAP_DENYWRITE //这个标志被忽略。
+ MAP_EXECUTABLE //同上
+ MAP_NORESERVE //不要为这个映射保留交换空间。当交换空间被保留，对映射区修改的可能会得到保证。当交换空间不被保留，同时内存不足，对映射区的修改会引起段违例信号。
+ MAP_LOCKED //锁定映射区的页面，从而防止页面被交换出内存。
+ MAP_GROWSDOWN //用于堆栈，告诉内核VM系统，映射区可以向下扩展。
+ MAP_ANONYMOUS //匿名映射，映射区不与任何文件关联。
+ MAP_ANON //MAP_ANONYMOUS的别称，不再被使用。
+ MAP_FILE //兼容标志，被忽略。
+ MAP_32BIT //将映射区放在进程地址空间的低2GB，MAP_FIXED指定时会被忽略。当前这个标志只在x86-64平台上得到支持。
+ MAP_POPULATE //为文件映射通过预读的方式准备好页表。随后对映射区的访问不会被页违例阻塞。
+ MAP_NONBLOCK //仅和MAP_POPULATE一起使用时才有意义。不执行预读，只为已存在于内存中的页面建立页表入口。


# 分配
## 对象是分配在堆上还是栈上？
Golang 编译器会将函数的局部变量分配到函数栈帧（stack frame）上。然而，如果编译器不能确保变量在函数 return 之后不再被引用，编译器就会将变量分配到堆上。而且，如果一个局部变量非常大，那么它也应该被分配到堆上而不是栈上，如果一个变量被取地址，那么它就有可能被分配到堆上。然而，还要对这些变量做逃逸分析，如果函数 return 之后，变量不再被引用，则将其分配到栈上。

例如：

```go
package main


func test()*int{
	x := new(int)
	*x = 0xAABB
	return x
}


func main() {
	println(test())
}


```
$ go build -gcflags "-l" -o te r.go //关闭内联优化

```s
TEXT main.test(SB) go/src/r.go
  r.go:4                0x104df10               65488b0c2530000000      MOVQ GS:0x30, CX                        
  r.go:4                0x104df19               483b6110                CMPQ 0x10(CX), SP                       
  r.go:4                0x104df1d               7639                    JBE 0x104df58                           
  r.go:4                0x104df1f               4883ec18                SUBQ $0x18, SP                          
  r.go:4                0x104df23               48896c2410              MOVQ BP, 0x10(SP)                       
  r.go:4                0x104df28               488d6c2410              LEAQ 0x10(SP), BP                       
  r.go:5                0x104df2d               488d054c9d0000          LEAQ type.*+40032(SB), AX               
  r.go:5                0x104df34               48890424                MOVQ AX, 0(SP)                          
  r.go:5                0x104df38               e813c1fbff              CALL runtime.newobject(SB)    //在堆上分配           
  r.go:5                0x104df3d               488b442408              MOVQ 0x8(SP), AX                        
  r.go:6                0x104df42               48c700bbaa0000          MOVQ $0xaabb, 0(AX)                     
  r.go:7                0x104df49               4889442420              MOVQ AX, 0x20(SP)                       
  r.go:7                0x104df4e               488b6c2410              MOVQ 0x10(SP), BP                       
  r.go:7                0x104df53               4883c418                ADDQ $0x18, SP                          
  r.go:7                0x104df57               c3                      RET                                     
  r.go:4                0x104df58               e85389ffff              CALL runtime.morestack_noctxt(SB)       
  r.go:4                0x104df5d               ebb1                    JMP main.test(SB)                       
  :-1                   0x104df5f               cc                      INT $0x3         
```
##### newObject实现
newObject也是内置new函数的时实现
```go
//新内置编译器（前端和SSA后端）的实现知道此函数的签名
func newobject(typ *_type) unsafe.Pointer {
	return mallocgc(typ.size, typ, true)
}
//分配大小为bytes的对象。从per-P缓存的空闲列表中分配小对象。大型对象（> 32 kB）直接从堆中分配
func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {
	if gcphase == _GCmarktermination {
		throw("mallocgc called with gcphase == _GCmarktermination")
	}

	if size == 0 {
		return unsafe.Pointer(&zerobase)
	}

	if debug.sbrk != 0 {
		align := uintptr(16)
		if typ != nil {
			align = uintptr(typ.align)
		}
		return persistentalloc(size, align, &memstats.other_sys)
	}

	// assistG is the G to charge for this allocation, or nil if
	// GC is not currently active.
	var assistG *g
	if gcBlackenEnabled != 0 {
		// Charge the current user G for this allocation.
		assistG = getg()
		if assistG.m.curg != nil {
			assistG = assistG.m.curg
		}
		// Charge the allocation against the G. We'll account
		// for internal fragmentation at the end of mallocgc.
		assistG.gcAssistBytes -= int64(size)

		if assistG.gcAssistBytes < 0 {
			// This G is in debt. Assist the GC to correct
			// this before allocating. This must happen
			// before disabling preemption.
			gcAssistAlloc(assistG)
		}
	}

	// Set mp.mallocing to keep from being preempted by GC.
	mp := acquirem()
	if mp.mallocing != 0 {
		throw("malloc deadlock")
	}
	if mp.gsignal == getg() {
		throw("malloc during signal")
	}
	mp.mallocing = 1

	shouldhelpgc := false
	dataSize := size
	c := gomcache()
	var x unsafe.Pointer
	noscan := typ == nil || typ.kind&kindNoPointers != 0
	if size <= maxSmallSize {
		//不需要扫描非指针的小对象
		if noscan && size < maxTinySize {
			// Tiny allocator.
			//
			// Tiny allocator将几个微小的分配请求组合到一个内存块中。当所有子对象都无法访问时，将释放生成的内存块。子对象必须是noscan（没有指针），这可以确保可能浪费的内存量受到限制。
			// 用于组合的内存块的大小（maxTinySize）是可调的。当前设置是16个字节，这与2倍最坏情况的内存浪费（当除了一个子对象之外的所有子对象都不可到达时）有关。 8个字节将导致完全没有浪费，但提供更少的组合机会.32个字节提供更多的组合机会，但可能导致4倍最坏情况下的浪费。无论块大小如何，最佳案例为8倍。
			// 
			// 从Tiny allocator获得的对象不能被显式释放。因此，当一个对象被显式释放时，确保它的大小> = maxTinySize。
			//
			// SetFinalizer对于可能来自Tiny allocator的对象有一个特殊情况，它允许为内存块的内部字节设置终结器。
			//
			// Tiny allocator的主要目标是小字符串和独立的转义变量。在json基准测试中，分配器将分配数量减少了大约12％，并将堆大小减少了大约20％。
			off := c.tinyoffset
			// 将指针进行内存对齐
			if size&7 == 0 {
				off = round(off, 8)
			} else if size&3 == 0 {
				off = round(off, 4)
			} else if size&1 == 0 {
				off = round(off, 2)
			}
			// 如果空间足够
			if off+size <= maxTinySize && c.tiny != 0 {
				// The object fits into existing tiny block.
				//返回指针调整偏移量为下次分配作准备
				x = unsafe.Pointer(c.tiny + off)
				c.tinyoffset = off + size
				c.local_tinyallocs++
				mp.mallocing = 0
				releasem(mp)
				return x
			}
			// Allocate a new maxTinySize block.
			//分配新的tiny块，从sizeclass=2的span.freelist获得一个16字节的object
			span := c.alloc[tinySpanClass]
			v := nextFreeFast(span)
			if v == 0 {
				v, _, shouldhelpgc = c.nextFree(tinySpanClass)
			}
			x = unsafe.Pointer(v)
			(*[2]uint64)(x)[0] = 0
			(*[2]uint64)(x)[1] = 0
			// See if we need to replace the existing tiny block with the new one
			// based on amount of remaining free space.
			if size < c.tinyoffset || c.tiny == 0 {
				c.tiny = uintptr(x)
				c.tinyoffset = size
			}
			size = maxTinySize
		} else {
			//普通小对象
			var sizeclass uint8
			if size <= smallSizeMax-8 {
				sizeclass = size_to_class8[(size+smallSizeDiv-1)/smallSizeDiv]
			} else {
				sizeclass = size_to_class128[(size-smallSizeMax+largeSizeDiv-1)/largeSizeDiv]
			}
			size = uintptr(class_to_size[sizeclass])
			spc := makeSpanClass(sizeclass, noscan)
			// 从对应规格的span.freelist获取object
			span := c.alloc[spc]
			v := nextFreeFast(span)
			if v == 0 {
				v, span, shouldhelpgc = c.nextFree(spc)
			}
			x = unsafe.Pointer(v)
			if needzero && span.needzero != 0 {
				memclrNoHeapPointers(unsafe.Pointer(v), size)
			}
		}
	} else { //大对象直接从heap分配span
		var s *mspan
		shouldhelpgc = true
		systemstack(func() {
			s = largeAlloc(size, needzero, noscan)
		})
		s.freeindex = 1
		s.allocCount = 1
		x = unsafe.Pointer(s.base())
		size = s.elemsize
	}

	var scanSize uintptr
	if !noscan {
		// 如果分配一个defer + arg块，现在我们选择了一个足够大的malloc大小来容纳所有内容，将“ask for”大小缩减到只有defer头，这样GC位图会将arg块记录为不包含任何内容完全（好像它是由大小舍入引起的malloc块末尾的未使用空间）。延迟arg区域作为scanstack的一部分进行扫描。
		if typ == deferType {
			dataSize = unsafe.Sizeof(_defer{})
		}
		heapBitsSetType(uintptr(x), size, dataSize, typ)
		if dataSize > typ.size {
			// Array allocation. If there are any
			// pointers, GC has to scan to the last
			// element.
			if typ.ptrdata != 0 {
				scanSize = dataSize - typ.size + typ.ptrdata
			}
		} else {
			scanSize = typ.ptrdata
		}
		c.local_scan += scanSize
	}

	// Ensure that the stores above that initialize x to
	// type-safe memory and set the heap bits occur before
	// the caller can make x observable to the garbage
	// collector. Otherwise, on weakly ordered machines,
	// the garbage collector could follow a pointer to x,
	// but see uninitialized memory or stale heap bits.
	publicationBarrier()

	// Allocate black during GC.
	// All slots hold nil so no scanning is needed.
	// This may be racing with GC so do it atomically if there can be
	// a race marking the bit.
	if gcphase != _GCoff {
		gcmarknewobject(uintptr(x), size, scanSize)
	}

	if raceenabled {
		racemalloc(x, size)
	}

	if msanenabled {
		msanmalloc(x, size)
	}

	mp.mallocing = 0
	releasem(mp)

	if debug.allocfreetrace != 0 {
		tracealloc(x, size, typ)
	}

	if rate := MemProfileRate; rate > 0 {
		if size < uintptr(rate) && int32(size) < c.next_sample {
			c.next_sample -= int32(size)
		} else {
			mp := acquirem()
			profilealloc(mp, x, size)
			releasem(mp)
		}
	}

	if assistG != nil {
		// Account for internal fragmentation in the assist
		// debt now that we know it.
		assistG.gcAssistBytes -= int64(size - dataSize)
	}

	if shouldhelpgc {
		if t := (gcTrigger{kind: gcTriggerHeap}); t.test() {
			gcStart(gcBackgroundMode, t)
		}
	}

	return x
}
```

基本思路:
1. 大对象直接从heap获取span
2. 小对象从cache.alloc[spanclass].freelist获取object
3. 微小对象组合使用cache.tiny 获得object

微小对象不能为指针，因为多个小对象组合到一个object中，它从span.list获取一个16字节的object，然后利用偏移量记录下次分配位置
## 分配大对象

```go
func largeAlloc(size uintptr, needzero bool, noscan bool) *mspan {
	// print("largeAlloc size=", size, "\n")

	if size+_PageSize < size {
		throw("out of memory")
	}
	npages := size >> _PageShift
	if size&_PageMask != 0 {
		npages++
	}

	// Deduct credit for this span allocation and sweep if
	// necessary. mHeap_Alloc will also sweep npages, so this only
	// pays the debt down to npage pages.
	deductSweepCredit(npages*_PageSize, npages)

	s := mheap_.alloc(npages, makeSpanClass(0, noscan), true, needzero)
	if s == nil {
		throw("out of memory")
	}
	s.limit = s.base() + size
	heapBitsForAddr(s.base()).initSpan(s)
	return s
}
```